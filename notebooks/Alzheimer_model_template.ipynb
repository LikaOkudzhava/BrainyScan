{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import cv2\n",
    "import zipfile\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(src_path: str, zip_file: zipfile.ZipFile):\n",
    "    ''' add dirctory with relative path to the zip archive\n",
    "\n",
    "    Args:\n",
    "      src_path: path to the directory\n",
    "      zip_file: zip archive\n",
    "    '''\n",
    "    for root, dirs, files in os.walk(src_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, src_path)\n",
    "            zip_file.write(file_path, arcname)\n",
    "\n",
    "def archive_directory(src_dir: str, dst_file: str):\n",
    "    ''' archive directory\n",
    "\n",
    "    Args:\n",
    "      src_dir: path to the directory\n",
    "      dst_file: name of the archive\n",
    "    '''\n",
    "    with zipfile.ZipFile(dst_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipdir(src_dir, zipf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLAB = False\n",
    "\n",
    "\n",
    "MODEL_NAME = 'Template'\n",
    "SEED = 42\n",
    "image_size = (224, 224)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    USE_COLAB = True\n",
    "except ImportError:\n",
    "    USE_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join('../', 'data', 'SmallPreprocessed')\n",
    "\n",
    "if USE_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    filename = 'smallpreprocessed.zip'\n",
    "\n",
    "    dest_path = f'{filename}_extracted'\n",
    "    shutil.rmtree(dest_path, ignore_errors=True)\n",
    "\n",
    "    with zipfile.ZipFile( os.path.join('/content/drive/MyDrive', filename), 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_path)\n",
    "\n",
    "\n",
    "    dataset_dir = os.path.join('/content/', dest_path, 'data', 'SmallPreprocessed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function= tf.keras.applications.resnet.preprocess_input,\n",
    "    rescale=1./255,\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_dir, 'train'),\n",
    "    target_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_dir, 'test'),\n",
    "    target_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_dir, 'val'),\n",
    "    target_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = list(train_generator.class_indices.keys())\n",
    "class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Input(shape=(image_size[0], image_size[1], 3)))\n",
    "model.add(\n",
    "    tf.keras.applications.VGG16(\n",
    "        include_top = False,\n",
    "        pooling = 'avg',\n",
    "        weights = 'imagenet'))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(2048, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Build the model with the correct input shape\n",
    "model.build(input_shape=(None, image_size[0], image_size[1], 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = ModelCheckpoint(\n",
    "    f\"model_{MODEL_NAME}_checkpoint.keras\",\n",
    "    save_best_only = True)\n",
    "\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    patience = 10,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = val_generator,\n",
    "    callbacks = [checkpoint_cb, early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    model_dir = os.path.join('/content/', 'models')\n",
    "    zip_name = os.path.join('/content/', f'{MODEL_NAME}_model.zip')\n",
    "    gdrive_file = os.path.join('/content/drive/MyDrive/', f'{MODEL_NAME}_model.zip')\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model.save(os.path.join(model_dir, f'{MODEL_NAME}_model.keras'))\n",
    "    model.export(os.path.join(model_dir, MODEL_NAME))\n",
    "    archive_directory(model_dir, zip_name)\n",
    "\n",
    "    if os.path.exists(zip_name):\n",
    "        os.remove(zip_name)\n",
    "\n",
    "    archive_directory(model_dir, zip_name)\n",
    "\n",
    "    shutil.copyfile(zip_name, gdrive_file)\n",
    "\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    os.remove(zip_name)\n",
    "\n",
    "    print(f\"model data was stored on google drive as {gdrive_file}\")\n",
    "else:\n",
    "    model.save(f'../models/{MODEL_NAME}_model.keras')\n",
    "    model.export(f'../models/{MODEL_NAME}_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc= model.evaluate(test_generator)\n",
    "print('Val Loss =', score)\n",
    "print('Val Accuracy =', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_=pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist_['loss'],label='Train_Loss')\n",
    "plt.plot(hist_['val_loss'],label='Validation_Loss')\n",
    "plt.title('Train_Loss & Validation_Loss',fontsize=20)\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist_['accuracy'],label='Train_Accuracy')\n",
    "plt.plot(hist_['val_accuracy'],label='Validation_Accuracy')\n",
    "plt.title('Train_Accuracy & Validation_Accuracy',fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =test_generator.classes\n",
    "predictions = model.predict(test_generator)\n",
    "y_pred = np.argmax(predictions,axis=1)\n",
    "y_test = np.ravel(y_test)\n",
    "y_pred = np.ravel(y_pred)\n",
    "df = pd.DataFrame({'Actual': y_test, 'Prediction': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(y_test,y_pred)\n",
    "CM_percent = CM.astype('float') / CM.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(CM_percent,fmt='g',center = True,cbar=False,annot=True,cmap='Blues',xticklabels=class_num, yticklabels=class_num)\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationReport = classification_report(y_test,y_pred,target_names=class_num)\n",
    "print('Classification Report is : ', ClassificationReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
