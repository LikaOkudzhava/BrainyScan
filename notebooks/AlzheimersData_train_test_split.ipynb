{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZbCmH_50s0i",
        "outputId": "c5a56737-39fc-4fe8-c968-179c929db793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kagglehub\n",
            "  Using cached kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: packaging in /home/pirahna/projects/data_science/BrainyScan/.venv/lib/python3.11/site-packages (from kagglehub) (25.0)\n",
            "Collecting pyyaml (from kagglehub)\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests (from kagglehub)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm (from kagglehub)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->kagglehub)\n",
            "  Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->kagglehub)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->kagglehub)\n",
            "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->kagglehub)\n",
            "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Using cached kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
            "Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: urllib3, tqdm, pyyaml, idna, charset-normalizer, certifi, requests, kagglehub\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [kagglehub]━\u001b[0m \u001b[32m6/8\u001b[0m [requests]\n",
            "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 kagglehub-0.3.12 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yS5XstwhIFfb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import kagglehub\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "from tqdm import tqdm\n",
        "# from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-oyuo0qk1Uj0"
      },
      "outputs": [],
      "source": [
        "def count_files_in_dir(dir_name: str) -> str:\n",
        "    \"\"\"Counts the number of files in a directory\"\"\"\n",
        "    return len(\n",
        "        [f for f in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, f))]\n",
        "    )\n",
        "\n",
        "def get_split_pos(n_items: int, train: float, test: float = -1.) -> tuple[int, int]:\n",
        "    \"\"\"makes train-test split positions. if 'test' fraction not specified,\n",
        "    then all the items out of the 'train' set will be used as 'test' set.\n",
        "    otherwise, rest of the items can be used as 'validation' set\n",
        "    \"\"\"\n",
        "    n_train = int(n_items * train)\n",
        "    return (n_train, int(n_items * test) if test > 0 else n_items - n_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGm129gp-kaG"
      },
      "source": [
        "Kagglehub package updates requires no credentials to download a free dataset, updates dataset automated, not download it again, if it wasn't changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-LOmxbrIK7o"
      },
      "source": [
        "Google colab has it's own storage space, accessible by path '/content'. This space is very fast, quite huge, but will be deleted as session will close. Google Drive disk can be connected to this space and them will be available as directory. But google drive is relatively slow to operate from collab and has limit of th einput-output operations for one day. solution is: use collab own space for all file operations. if we need a file from GDrive, mount it and copy to the collab space, if we want to save something for a while, mount GDrive and copy file there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-e8wzUGJAPK"
      },
      "source": [
        "Here I will collect all the data in colab space, will create a tar.gz file with train-test-split and will copy the result to the GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IiB5WC7FJL_q"
      },
      "outputs": [],
      "source": [
        "# os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlTcFlFm0mfb",
        "outputId": "6482d7fc-9181-4d24-9ef5-220808f8619f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming download from 203423744 bytes (213746835 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/aryansinghal10/alzheimers-multiclass-dataset-equal-and-augmented?dataset_version_number=1 (203423744/417170579) bytes left.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 398M/398M [00:37<00:00, 5.63MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files in NonDemented: 12800\n",
            "Number of files in MildDemented: 10000\n",
            "Number of files in ModerateDemented: 10000\n",
            "Number of files in VeryMildDemented: 11200\n"
          ]
        }
      ],
      "source": [
        "original_dataset = os.path.join(\n",
        "    kagglehub.dataset_download(\"aryansinghal10/alzheimers-multiclass-dataset-equal-and-augmented\"),\n",
        "    'combined_images'\n",
        ")\n",
        "\n",
        "original_dirs = (\n",
        "    ('NonDemented', os.path.join(original_dataset, 'NonDemented')),\n",
        "    ('MildDemented', os.path.join(original_dataset, 'MildDemented')),\n",
        "    ('ModerateDemented', os.path.join(original_dataset, 'ModerateDemented')),\n",
        "    ('VeryMildDemented', os.path.join(original_dataset, 'VeryMildDemented')),\n",
        ")\n",
        "\n",
        "for name, f_dir in original_dirs:\n",
        "    file_count = count_files_in_dir(f_dir)\n",
        "    print(f\"Number of files in {name}: {file_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXeU7-Xrfkk-",
        "outputId": "faf23f0a-ebe6-4bfa-df08-ee2afbad80a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copy train set, VeryMildDemented files: 100%|██████████| 7839/7839 [00:01<00:00, 5500.40it/s]\n",
            "Copy val set, VeryMildDemented files: 100%|██████████| 1681/1681 [00:00<00:00, 5379.07it/s]\n",
            "Copy test set, VeryMildDemented files: 100%|██████████| 1680/1680 [00:00<00:00, 5580.15it/s]\n",
            "Copy train set, NonDemented files: 100%|██████████| 8960/8960 [00:01<00:00, 5433.33it/s]\n",
            "Copy val set, NonDemented files: 100%|██████████| 1920/1920 [00:00<00:00, 5439.94it/s]\n",
            "Copy test set, NonDemented files: 100%|██████████| 1920/1920 [00:00<00:00, 5386.53it/s]\n",
            "Copy train set, MildDemented files: 100%|██████████| 7000/7000 [00:01<00:00, 5320.56it/s]\n",
            "Copy val set, MildDemented files: 100%|██████████| 1500/1500 [00:00<00:00, 5609.37it/s]\n",
            "Copy test set, MildDemented files: 100%|██████████| 1500/1500 [00:00<00:00, 5358.02it/s]\n",
            "Copy train set, ModerateDemented files: 100%|██████████| 7000/7000 [00:01<00:00, 5297.67it/s]\n",
            "Copy val set, ModerateDemented files: 100%|██████████| 1500/1500 [00:00<00:00, 5399.09it/s]\n",
            "Copy test set, ModerateDemented files: 100%|██████████| 1500/1500 [00:00<00:00, 5426.51it/s]\n"
          ]
        }
      ],
      "source": [
        "output_base = 'AlzheimersData_Split'\n",
        "\n",
        "classes = os.listdir(original_dataset)\n",
        "\n",
        "# remove files from the previous runs\n",
        "shutil.rmtree(output_base, ignore_errors=True)\n",
        "\n",
        "# Ensure output folders exist\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(output_base, split, cls), exist_ok=True)\n",
        "\n",
        "# Split and copy files\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(original_dataset, cls)\n",
        "\n",
        "    images = os.listdir(cls_path)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    n_train, n_test = get_split_pos(len(images), 0.7, 0.15)\n",
        "\n",
        "    train_imgs = images[ : n_train]\n",
        "    test_imgs = images[n_train : n_train + n_test]\n",
        "    val_imgs = images[n_train + n_test : ]\n",
        "\n",
        "    for img_list, split in zip([train_imgs, val_imgs, test_imgs], ['train', 'val', 'test']):\n",
        "        for img in tqdm(img_list, desc=f'Copy {split} set, {cls} files'):\n",
        "            src = os.path.join(cls_path, img)\n",
        "            dst = os.path.join(output_base, split, cls, img)\n",
        "            shutil.copy(src, dst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk-B_hgQrktk",
        "outputId": "9185fba6-260b-494f-eb5f-b8208b60b58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files in train set:\n",
            "    VeryMildDemented: 7000\n",
            "    VeryMildDemented: 7000\n",
            "    VeryMildDemented: 8960\n",
            "    VeryMildDemented: 7839\n",
            "  Total: 30799\n",
            "\n",
            "Number of files in test set:\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1920\n",
            "    VeryMildDemented: 1680\n",
            "  Total: 6600\n",
            "\n",
            "Number of files in val set:\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1920\n",
            "    VeryMildDemented: 1681\n",
            "  Total: 6601\n",
            "\n"
          ]
        }
      ],
      "source": [
        "split_set_dirs = {\n",
        "    'train': os.path.join(output_base, 'train'),\n",
        "    'test': os.path.join(output_base, 'test'),\n",
        "    'val': os.path.join(output_base, 'val')\n",
        "}\n",
        "\n",
        "for split_name, split_dir in split_set_dirs.items():\n",
        "    count = 0\n",
        "    print(f\"Number of files in {split_name} set:\")\n",
        "    for cls in ('MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented'):\n",
        "        file_count = count_files_in_dir(os.path.join(split_dir, cls))\n",
        "        count += file_count\n",
        "        print(f\"    {name}: {file_count}\")\n",
        "\n",
        "    print(f\"  Total: {count}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiOn4JI-9DcB",
        "outputId": "58f4bf93-39e0-4dd7-e01a-9d02d57c7584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Folder zipped successfully!\n"
          ]
        }
      ],
      "source": [
        "def zip_folder(folder_path, output_path):\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Store relative path inside the zip\n",
        "                arcname = os.path.relpath(file_path, start=folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "# Example usage:\n",
        "folder_to_zip = 'AlzheimersData_Split'\n",
        "output_zip_path = 'AlzheimersData_Split.zip'\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip_path)\n",
        "print(\"✅ Folder zipped successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWNxV3M0MF5_",
        "outputId": "2ba137e9-d64f-4abf-9a11-c3f52aa13fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "shutil.copy('AlzheimersData_Split.zip', '/content/drive/MyDrive/AlzheimersData_Split.zip')\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
