{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZbCmH_50s0i",
        "outputId": "c5a56737-39fc-4fe8-c968-179c929db793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS5XstwhIFfb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import kagglehub\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oyuo0qk1Uj0"
      },
      "outputs": [],
      "source": [
        "def count_files_in_dir(dir_name: str) -> str:\n",
        "    \"\"\"Counts the number of files in a directory\"\"\"\n",
        "    return len(\n",
        "        [f for f in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, f))]\n",
        "    )\n",
        "\n",
        "def get_split_pos(n_items: int, train: float, test: float = -1.) -> tuple[int, int]:\n",
        "    \"\"\"makes train-test split positions. if 'test' fraction not specified,\n",
        "    then all the items out of the 'train' set will be used as 'test' set.\n",
        "    otherwise, rest of the items can be used as 'validation' set\n",
        "    \"\"\"\n",
        "    n_train = int(n_items * train)\n",
        "    return (n_train, int(n_items * test) if test > 0 else n_items - n_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGm129gp-kaG"
      },
      "source": [
        "Kagglehub package updates requires no credentials to download a free dataset, updates dataset automated, not download it again, if it wasn't changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-LOmxbrIK7o"
      },
      "source": [
        "Google colab has it's own storage space, accessible by path '/content'. This space is very fast, quite huge, but will be deleted as session will close. Google Drive disk can be connected to this space and them will be available as directory. But google drive is relatively slow to operate from collab and has limit of th einput-output operations for one day. solution is: use collab own space for all file operations. if we need a file from GDrive, mount it and copy to the collab space, if we want to save something for a while, mount GDrive and copy file there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-e8wzUGJAPK"
      },
      "source": [
        "Here I will collect all the data in colab space, will create a tar.gz file with train-test-split and will copy the result to the GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiB5WC7FJL_q"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlTcFlFm0mfb",
        "outputId": "6482d7fc-9181-4d24-9ef5-220808f8619f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files in NonDemented: 12800\n",
            "Number of files in MildDemented: 10000\n",
            "Number of files in ModerateDemented: 10000\n",
            "Number of files in VeryMildDemented: 11200\n"
          ]
        }
      ],
      "source": [
        "original_dataset = os.path.join(\n",
        "    kagglehub.dataset_download(\"aryansinghal10/alzheimers-multiclass-dataset-equal-and-augmented\"),\n",
        "    'combined_images'\n",
        ")\n",
        "\n",
        "original_dirs = (\n",
        "    ('NonDemented', os.path.join(original_dataset, 'NonDemented')),\n",
        "    ('MildDemented', os.path.join(original_dataset, 'MildDemented')),\n",
        "    ('ModerateDemented', os.path.join(original_dataset, 'ModerateDemented')),\n",
        "    ('VeryMildDemented', os.path.join(original_dataset, 'VeryMildDemented')),\n",
        ")\n",
        "\n",
        "for name, f_dir in original_dirs:\n",
        "    file_count = count_files_in_dir(f_dir)\n",
        "    print(f\"Number of files in {name}: {file_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXeU7-Xrfkk-",
        "outputId": "faf23f0a-ebe6-4bfa-df08-ee2afbad80a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copy train set, ModerateDemented files: 100%|██████████| 7000/7000 [00:24<00:00, 287.69it/s]\n",
            "Copy val set, ModerateDemented files: 100%|██████████| 1500/1500 [00:05<00:00, 273.34it/s]\n",
            "Copy test set, ModerateDemented files: 100%|██████████| 1500/1500 [00:05<00:00, 272.15it/s]\n",
            "Copy train set, NonDemented files: 100%|██████████| 8960/8960 [00:31<00:00, 281.73it/s]\n",
            "Copy val set, NonDemented files: 100%|██████████| 1920/1920 [00:07<00:00, 244.61it/s]\n",
            "Copy test set, NonDemented files: 100%|██████████| 1920/1920 [00:07<00:00, 240.29it/s]\n",
            "Copy train set, VeryMildDemented files: 100%|██████████| 7839/7839 [00:24<00:00, 324.66it/s]\n",
            "Copy val set, VeryMildDemented files: 100%|██████████| 1681/1681 [00:05<00:00, 322.12it/s]\n",
            "Copy test set, VeryMildDemented files: 100%|██████████| 1680/1680 [00:04<00:00, 349.47it/s]\n",
            "Copy train set, MildDemented files: 100%|██████████| 7000/7000 [00:09<00:00, 717.21it/s]\n",
            "Copy val set, MildDemented files: 100%|██████████| 1500/1500 [00:01<00:00, 791.81it/s]\n",
            "Copy test set, MildDemented files: 100%|██████████| 1500/1500 [00:02<00:00, 678.56it/s]\n"
          ]
        }
      ],
      "source": [
        "output_base = 'AlzheimersData_Split'\n",
        "\n",
        "classes = os.listdir(original_dataset)\n",
        "\n",
        "# remove files from the previous runs\n",
        "shutil.rmtree(output_base, ignore_errors=True)\n",
        "\n",
        "# Ensure output folders exist\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(output_base, split, cls), exist_ok=True)\n",
        "\n",
        "# Split and copy files\n",
        "for cls in classes:\n",
        "    cls_path = os.path.join(original_dataset, cls)\n",
        "\n",
        "    images = os.listdir(cls_path)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    n_train, n_test = get_split_pos(len(images), 0.7, 0.15)\n",
        "\n",
        "    train_imgs = images[ : n_train]\n",
        "    test_imgs = images[n_train : n_train + n_test]\n",
        "    val_imgs = images[n_train + n_test : ]\n",
        "\n",
        "    for img_list, split in zip([train_imgs, val_imgs, test_imgs], ['train', 'val', 'test']):\n",
        "        for img in tqdm(img_list, desc=f'Copy {split} set, {cls} files'):\n",
        "            src = os.path.join(cls_path, img)\n",
        "            dst = os.path.join(output_base, split, cls, img)\n",
        "            shutil.copy(src, dst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk-B_hgQrktk",
        "outputId": "9185fba6-260b-494f-eb5f-b8208b60b58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files in train set:\n",
            "    VeryMildDemented: 7000\n",
            "    VeryMildDemented: 7000\n",
            "    VeryMildDemented: 8960\n",
            "    VeryMildDemented: 7839\n",
            "  Total: 30799\n",
            "\n",
            "Number of files in test set:\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1920\n",
            "    VeryMildDemented: 1680\n",
            "  Total: 6600\n",
            "\n",
            "Number of files in val set:\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1500\n",
            "    VeryMildDemented: 1920\n",
            "    VeryMildDemented: 1681\n",
            "  Total: 6601\n",
            "\n"
          ]
        }
      ],
      "source": [
        "split_set_dirs = {\n",
        "    'train': os.path.join(output_base, 'train'),\n",
        "    'test': os.path.join(output_base, 'test'),\n",
        "    'val': os.path.join(output_base, 'val')\n",
        "}\n",
        "\n",
        "for split_name, split_dir in split_set_dirs.items():\n",
        "    count = 0\n",
        "    print(f\"Number of files in {split_name} set:\")\n",
        "    for cls in ('MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented'):\n",
        "        file_count = count_files_in_dir(os.path.join(split_dir, cls))\n",
        "        count += file_count\n",
        "        print(f\"    {name}: {file_count}\")\n",
        "\n",
        "    print(f\"  Total: {count}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiOn4JI-9DcB",
        "outputId": "58f4bf93-39e0-4dd7-e01a-9d02d57c7584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Folder zipped successfully!\n"
          ]
        }
      ],
      "source": [
        "def zip_folder(folder_path, output_path):\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Store relative path inside the zip\n",
        "                arcname = os.path.relpath(file_path, start=folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "# Example usage:\n",
        "folder_to_zip = 'AlzheimersData_Split'\n",
        "output_zip_path = 'AlzheimersData_Split.zip'\n",
        "\n",
        "zip_folder(folder_to_zip, output_zip_path)\n",
        "print(\"✅ Folder zipped successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWNxV3M0MF5_",
        "outputId": "2ba137e9-d64f-4abf-9a11-c3f52aa13fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "shutil.copy('AlzheimersData_Split.zip', '/content/drive/MyDrive/AlzheimersData_Split.zip')\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
